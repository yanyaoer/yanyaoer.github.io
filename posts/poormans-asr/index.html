<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>poorman's ASR solution | yányào.com</title><link rel=stylesheet href=/css/main.min.6f828ddf68bbacf732eaa73b52e82f5d9de0e36e874ab957dc8b22dac66d9fa7.css integrity="sha256-b4KN32i7rPcy6qc7UugvXZ3g426HSrlX3Isi2sZtn6c=" crossorigin=anonymous></head><body><header><header><a href=https://xn--ynyo-2nad.com/ class=title><h2>yányào.com</h2></a><nav><a href=/>Home</a>
<a href=/posts>Posts</a>
<a href=/index.xml>RSS</a></nav></header><main><h1>poorman's ASR solution</h1><time datetime=2025-05-11T17:50:56+08:00>May 11, 2025</time><h2 id=intro>Intro</h2><p>大概20年h2期间，听闻feishu支持了会议自动转译字幕功能，but
当时私有化部署的版本一直没有支持，于是私下找了语音助手团队的同学企图蹭服务用于自动生成会议纪要，<strong>懒即是原动力</strong></p><p>再 but 一下，出于服务成本考量没有蹭成功，后来对应的同学也陆续离职就没下文了</p><p>23年大模型开始各种火热起来，于是借助 openai 的 whisper
在本地陆续写了一些音频转文本的小任务，比如 yt-dlp 下载 bilibili
视频后转音频再转文本，个人习惯上阅读的效率还是高于视频形式的</p><ul><li>[scribe] <a href=https://gist.github.com/yanyaoer/5cc7b0dd6729f306ad3cb740d501cabd>https://gist.github.com/yanyaoer/5cc7b0dd6729f306ad3cb740d501cabd</a></li></ul><p>另外结合语音识别和 LLM 的发散，也试着搓了点语音助手的尝试</p><ul><li>[voice assistant with computer-use] <a href=https://gist.github.com/yanyaoer/752a73d2d6df5a2aa0de179a0f68e8a1>https://gist.github.com/yanyaoer/752a73d2d6df5a2aa0de179a0f68e8a1</a></li><li>[hook for whisper llm-talk] <a href=https://github.com/yanyaoer/whisper.cpp/commit/da85a1b4791e4a7eeb5adb31d47d4c5e53618234>https://github.com/yanyaoer/whisper.cpp/commit/da85a1b4791e4a7eeb5adb31d47d4c5e53618234</a></li></ul><p>但是实时语音识别场景，用 whiper-stream / sense-voice
这类方案还是会有识别不准确、内容丢失的问题存在，调整 step/length
参数效果也不明显，vad 方案延迟也很高&mldr;</p><h2 id=new-solution>New solution</h2><p>上周忽然想起了 kaldi 项目也蛮经典了，于是就试着用 sherpa-[ncnn|onnx] 尝试了下 demo
sherpa-[ncnn|onnx]-microphone
流式识别输出的整体效果已经很接近把音频文件喂给大模型的质量</p><p>于是联系维护者 @csukuangfj
请教怎么提升识别准确率，大佬过于卷周末还给造了新方案和输出优化，甚至还录了 b
站视频 <a href=https://www.bilibili.com/video/BV1xcEMz8EsX/>https://www.bilibili.com/video/BV1xcEMz8EsX/</a></p><iframe src="//player.bilibili.com/player.html?bvid=BV1xcEMz8EsX&page=1" scrolling=no autoplay border=0 frameborder=no framespacing=0 allowfullscreen></iframe><ul><li>[Add real-time speech recognition example for SenseVoice. (#2197)] <a href=https://github.com/k2-fsa/sherpa-onnx/commit/53518efd2fe70f49b86f180a4e5b49fdc374da82>https://github.com/k2-fsa/sherpa-onnx/commit/53518efd2fe70f49b86f180a4e5b49fdc374da82</a></li><li>[Fix displaying streaming speech recognition results for Python. (#2196)] <a href=https://github.com/k2-fsa/sherpa-onnx/commit/4a833a754743076d68252731c681511e2d9390bd>https://github.com/k2-fsa/sherpa-onnx/commit/4a833a754743076d68252731c681511e2d9390bd</a></li></ul><p>另外也体验到 two-pass 双模型识别的效果，两种方式都能满足俺的使用场景🎉</p><ul><li>[Two-pass real-time speech recognition from a microphone with sherpa-onnx] <a href=https://github.com/k2-fsa/sherpa-onnx/blob/b269e5cccc03c3e61dd9f363bb7b64b5b0ce6c3a/python-api-examples/two-pass-speech-recognition-from-microphone.py>https://github.com/k2-fsa/sherpa-onnx/blob/b269e5cccc03c3e61dd9f363bb7b64b5b0ce6c3a/python-api-examples/two-pass-speech-recognition-from-microphone.py</a></li></ul><p>在大佬的基础示例上，简单补充点内容的保存</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>          result = result.lower().strip()
</span></span><span style=display:flex><span>          display.update_text(result)
</span></span><span style=display:flex><span><span style=color:#a6e22e>+          log(result)
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>          display.finalize_current_sentence()
</span></span><span style=display:flex><span>          display.display()
</span></span><span style=display:flex><span>        else:
</span></span><span style=display:flex><span>          sample_buffers = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        first_recognizer.reset(stream)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>+ def log(result):
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+   global file_handler
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+   file_handler.write(f&#34;{datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)} - {result}\n&#34;)
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+   file_handler.flush()
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>if __name__ == &#34;__main__&#34;:
</span></span><span style=display:flex><span><span style=color:#a6e22e>+  from datetime import datetime
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+  ts = datetime.now().strftime(&#34;%y%m%d%H%M%S&#34;)
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+  file_path = Path(f&#34;~/Desktop/asr/2pass.{ts}.txt&#34;).expanduser()
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+  file_handler = open(file_path, &#34;a&#34;)
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>  try:
</span></span><span style=display:flex><span>    main()
</span></span><span style=display:flex><span>  except KeyboardInterrupt:
</span></span><span style=display:flex><span>    print(&#34;\nCaught Ctrl + C. Exiting&#34;)
</span></span><span style=display:flex><span><span style=color:#a6e22e>+  finally:
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+    file_handler.close()
</span></span></span></code></pre></div></main><footer><p>Copyright 2025. All rights reserved.</p></footer></body></html>